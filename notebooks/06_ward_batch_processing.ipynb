{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# [6] Wardバッチ処理 - Phase 3\n",
    "\n",
    "ミニマップキャプチャからward座標を抽出し、タイムラインデータと統合します。\n",
    "\n",
    "## 処理フロー\n",
    "1. **YOLO推論**: 全フレームでward検出（GPUバッチ推論対応）\n",
    "2. **クラスタリング**: 同一wardをグループ化\n",
    "3. **タイムライン統合**: Riot APIのwardイベントとマッチング\n",
    "\n",
    "## 入出力\n",
    "- **入力**: ミニマップ画像 (`C:\\dataset_20260105\\JP1-*\\0\\*.png`)\n",
    "- **出力**: \n",
    "  - `detections_raw.csv` - 生の検出結果\n",
    "  - `wards.csv` - クラスタリング後のward情報\n",
    "  - `wards_matched.csv` - タイムライン統合済み\n",
    "\n",
    "## 前提条件\n",
    "- Phase 2完了（YOLOv8モデル `models/best.pt`）\n",
    "- ミニマップキャプチャ完了（notebook 04）\n",
    "- タイムラインデータ（`data/timeline/*.json`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-1: セットアップ（インポート + 設定 + モデル読み込み）\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# プロジェクトルート\n",
    "PROJECT_ROOT = Path(r\"c:\\Users\\lapis\\Desktop\\LoL_WorkSp_win\\pyLoL-_WorkSp\\pyLoL-v2\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from autoLeague.dataset.ward_tracker import WardTracker\n",
    "\n",
    "# === パス設定 ===\n",
    "MODEL_PATH = PROJECT_ROOT / \"models\" / \"best.pt\"\n",
    "DATASET_DIR = Path(r\"C:\\dataset_20260105\")\n",
    "TIMELINE_DIR = PROJECT_ROOT / \"data\" / \"timeline\"\n",
    "\n",
    "# === 推論設定 ===\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "IMAGE_SIZE = 512\n",
    "\n",
    "# GPUバッチサイズ（VRAM容量に応じて自動調整）\n",
    "# RTX 3050 (4GB): 8, RTX 3080 (10GB): 32, A100 (40GB): 64\n",
    "if torch.cuda.is_available():\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    BATCH_SIZE = 8 if vram_gb < 6 else (32 if vram_gb < 16 else 64)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "else:\n",
    "    BATCH_SIZE = 1\n",
    "    print(\"GPU: 利用不可（CPUで実行）\")\n",
    "\n",
    "# === クラスタリング設定 ===\n",
    "DISTANCE_THRESHOLD = 0.01  # 同一wardと判定する座標距離\n",
    "MIN_FRAMES = 3  # ノイズ除去：最小連続フレーム数\n",
    "GAP_TOLERANCE = 10  # 検出が途切れても同一wardとみなすフレーム数\n",
    "\n",
    "# === モデル読み込み ===\n",
    "model = YOLO(str(MODEL_PATH))\n",
    "\n",
    "print(f\"\\nバッチサイズ: {BATCH_SIZE}\")\n",
    "print(f\"モデル: {MODEL_PATH.name}\")\n",
    "print(f\"クラス: {model.names}\")\n",
    "print(f\"\\nデータセット: {DATASET_DIR}\")\n",
    "if DATASET_DIR.exists():\n",
    "    match_dirs = sorted(DATASET_DIR.glob(\"JP1-*\"))\n",
    "    print(f\"試合数: {len(match_dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-2: 関数定義（データ構造 + 推論 + クラスタリング）\n",
    "\n",
    "@dataclass\n",
    "class Detection:\n",
    "    \"\"\"1フレームでの検出結果\"\"\"\n",
    "    frame: int\n",
    "    class_id: int\n",
    "    class_name: str\n",
    "    x: float  # 正規化座標 (0-1)\n",
    "    y: float\n",
    "    w: float\n",
    "    h: float\n",
    "    confidence: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Ward:\n",
    "    \"\"\"クラスタリング後のward\"\"\"\n",
    "    ward_id: int\n",
    "    class_id: int\n",
    "    class_name: str\n",
    "    x: float\n",
    "    y: float\n",
    "    frame_start: int\n",
    "    frame_end: int\n",
    "    detections: List[Detection] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def confidence_avg(self) -> float:\n",
    "        return sum(d.confidence for d in self.detections) / len(self.detections) if self.detections else 0.0\n",
    "\n",
    "    @property\n",
    "    def detection_count(self) -> int:\n",
    "        return len(self.detections)\n",
    "\n",
    "\n",
    "def run_inference(model: YOLO, match_dir: Path, conf: float = CONFIDENCE_THRESHOLD,\n",
    "                  batch_size: int = BATCH_SIZE) -> List[Detection]:\n",
    "    \"\"\"1試合分の全フレームをバッチ推論\"\"\"\n",
    "    frame_dir = match_dir / \"0\"\n",
    "    if not frame_dir.exists():\n",
    "        return []\n",
    "\n",
    "    frame_files = sorted(frame_dir.glob(\"*.png\"), key=lambda p: int(p.stem))\n",
    "    if not frame_files:\n",
    "        return []\n",
    "\n",
    "    detections: List[Detection] = []\n",
    "    total_frames = len(frame_files)\n",
    "\n",
    "    for i in range(0, total_frames, batch_size):\n",
    "        batch_files = frame_files[i:i+batch_size]\n",
    "        batch_paths = [str(p) for p in batch_files]\n",
    "        frame_nums = [int(p.stem) for p in batch_files]\n",
    "\n",
    "        results = model(batch_paths, imgsz=IMAGE_SIZE, conf=conf, verbose=False)\n",
    "\n",
    "        for frame_num, result in zip(frame_nums, results):\n",
    "            for box in result.boxes:\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = model.names[class_id]\n",
    "                x, y, w, h = box.xywhn[0].tolist()\n",
    "                confidence = float(box.conf[0])\n",
    "                detections.append(Detection(\n",
    "                    frame=frame_num, class_id=class_id, class_name=class_name,\n",
    "                    x=x, y=y, w=w, h=h, confidence=confidence\n",
    "                ))\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "def cluster_detections(detections: List[Detection]) -> List[Ward]:\n",
    "    \"\"\"検出結果をクラスタリングしてward単位にまとめる\"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "\n",
    "    sorted_detections = sorted(detections, key=lambda d: d.frame)\n",
    "    active_wards: Dict[int, List[Ward]] = defaultdict(list)\n",
    "    completed_wards: List[Ward] = []\n",
    "    next_ward_id = 1\n",
    "\n",
    "    for det in sorted_detections:\n",
    "        matched = False\n",
    "        for ward in active_wards[det.class_id]:\n",
    "            dist = np.sqrt((det.x - ward.detections[-1].x)**2 + (det.y - ward.detections[-1].y)**2)\n",
    "            if dist < DISTANCE_THRESHOLD and det.frame - ward.frame_end <= GAP_TOLERANCE:\n",
    "                ward.detections.append(det)\n",
    "                ward.frame_end = det.frame\n",
    "                ward.x = sum(d.x for d in ward.detections) / len(ward.detections)\n",
    "                ward.y = sum(d.y for d in ward.detections) / len(ward.detections)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            new_ward = Ward(\n",
    "                ward_id=next_ward_id, class_id=det.class_id, class_name=det.class_name,\n",
    "                x=det.x, y=det.y, frame_start=det.frame, frame_end=det.frame, detections=[det]\n",
    "            )\n",
    "            active_wards[det.class_id].append(new_ward)\n",
    "            next_ward_id += 1\n",
    "\n",
    "        current_frame = det.frame\n",
    "        for class_id in list(active_wards.keys()):\n",
    "            still_active = []\n",
    "            for ward in active_wards[class_id]:\n",
    "                if current_frame - ward.frame_end > GAP_TOLERANCE:\n",
    "                    completed_wards.append(ward)\n",
    "                else:\n",
    "                    still_active.append(ward)\n",
    "            active_wards[class_id] = still_active\n",
    "\n",
    "    for class_id in active_wards:\n",
    "        completed_wards.extend(active_wards[class_id])\n",
    "\n",
    "    filtered_wards = [w for w in completed_wards if w.detection_count >= MIN_FRAMES]\n",
    "    for i, ward in enumerate(sorted(filtered_wards, key=lambda w: w.frame_start), start=1):\n",
    "        ward.ward_id = i\n",
    "\n",
    "    return sorted(filtered_wards, key=lambda w: w.frame_start)\n",
    "\n",
    "\n",
    "def save_detections(detections: List[Detection], output_path: Path):\n",
    "    \"\"\"生の検出結果をCSVに保存\"\"\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['frame', 'class_id', 'class_name', 'x', 'y', 'w', 'h', 'confidence'])\n",
    "        for d in detections:\n",
    "            writer.writerow([d.frame, d.class_id, d.class_name,\n",
    "                           f\"{d.x:.6f}\", f\"{d.y:.6f}\", f\"{d.w:.6f}\", f\"{d.h:.6f}\",\n",
    "                           f\"{d.confidence:.4f}\"])\n",
    "\n",
    "\n",
    "def save_wards(wards: List[Ward], output_path: Path):\n",
    "    \"\"\"クラスタリング後のward情報をCSVに保存\"\"\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ward_id', 'class_id', 'class_name', 'x', 'y',\n",
    "                        'frame_start', 'frame_end', 'detection_count', 'confidence_avg'])\n",
    "        for w in wards:\n",
    "            writer.writerow([w.ward_id, w.class_id, w.class_name,\n",
    "                           f\"{w.x:.6f}\", f\"{w.y:.6f}\",\n",
    "                           w.frame_start, w.frame_end,\n",
    "                           w.detection_count, f\"{w.confidence_avg:.4f}\"])\n",
    "\n",
    "print(\"関数定義完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## オプション: テスト推論\n",
    "\n",
    "1試合100フレームでテスト推論を実行し、処理速度を確認します。  \n",
    "本番実行時はスキップ可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-3: テスト推論（オプション）\n",
    "match_dirs = sorted(DATASET_DIR.glob(\"JP1-*\"))\n",
    "if match_dirs:\n",
    "    test_match = match_dirs[0]\n",
    "    frame_dir = test_match / \"0\"\n",
    "    frame_files = sorted(frame_dir.glob(\"*.png\"), key=lambda p: int(p.stem))[:100]\n",
    "\n",
    "    print(f\"テスト: {test_match.name} (100フレーム)\")\n",
    "    print(f\"バッチサイズ: {BATCH_SIZE}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_detections = []\n",
    "\n",
    "    for i in range(0, len(frame_files), BATCH_SIZE):\n",
    "        batch_files = frame_files[i:i+BATCH_SIZE]\n",
    "        batch_paths = [str(p) for p in batch_files]\n",
    "        frame_nums = [int(p.stem) for p in batch_files]\n",
    "        results = model(batch_paths, imgsz=IMAGE_SIZE, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
    "        for frame_num, result in zip(frame_nums, results):\n",
    "            for box in result.boxes:\n",
    "                test_detections.append(Detection(\n",
    "                    frame=frame_num, class_id=int(box.cls[0]), class_name=model.names[int(box.cls[0])],\n",
    "                    x=box.xywhn[0][0].item(), y=box.xywhn[0][1].item(),\n",
    "                    w=box.xywhn[0][2].item(), h=box.xywhn[0][3].item(),\n",
    "                    confidence=float(box.conf[0])\n",
    "                ))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    fps = len(frame_files) / elapsed\n",
    "\n",
    "    test_wards = cluster_detections(test_detections)\n",
    "\n",
    "    print(f\"\\n処理速度: {fps:.1f} FPS\")\n",
    "    print(f\"検出数: {len(test_detections)} -> ward数: {len(test_wards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 全試合バッチ処理\n",
    "\n",
    "YOLO推論 -> クラスタリング -> タイムライン統合を1ループで実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-4: 全試合バッチ処理（YOLO推論 + クラスタリング + タイムライン統合）\n",
    "match_dirs = sorted(DATASET_DIR.glob(\"JP1-*\"))\n",
    "total_matches = len(match_dirs)\n",
    "\n",
    "# WardTracker初期化（ハンガリアン法使用）\n",
    "tracker = WardTracker(\n",
    "    timeline_dir=TIMELINE_DIR,\n",
    "    dataset_dir=DATASET_DIR,\n",
    "    use_hungarian=True,\n",
    ")\n",
    "\n",
    "print(f\"全{total_matches}試合の処理を開始\")\n",
    "print(f\"バッチサイズ: {BATCH_SIZE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 累積統計\n",
    "cumulative_matched = 0\n",
    "cumulative_total = 0\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, match_dir in enumerate(tqdm(match_dirs, desc=\"全体進捗\")):\n",
    "    match_id = match_dir.name\n",
    "\n",
    "    try:\n",
    "        # 1. YOLO推論\n",
    "        detections = run_inference(model, match_dir)\n",
    "        if not detections:\n",
    "            results.append({\"match\": match_id, \"detections\": 0, \"wards\": 0, \"matched\": 0})\n",
    "            continue\n",
    "\n",
    "        # 2. 生の検出結果を保存\n",
    "        save_detections(detections, match_dir / \"detections_raw.csv\")\n",
    "\n",
    "        # 3. クラスタリング\n",
    "        wards = cluster_detections(detections)\n",
    "        save_wards(wards, match_dir / \"wards.csv\")\n",
    "\n",
    "        # 4. タイムライン統合\n",
    "        matched_wards = tracker.process_match(match_id)\n",
    "\n",
    "        # 統計計算\n",
    "        matched_count = sum(1 for w in matched_wards if w.match_status == \"matched\")\n",
    "        total_count = len([w for w in matched_wards if w.match_status != \"detection_only\"])\n",
    "\n",
    "        cumulative_matched += matched_count\n",
    "        cumulative_total += total_count\n",
    "        match_rate = cumulative_matched / cumulative_total * 100 if cumulative_total > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            \"match\": match_id,\n",
    "            \"detections\": len(detections),\n",
    "            \"wards\": len(wards),\n",
    "            \"matched\": matched_count\n",
    "        })\n",
    "\n",
    "        # 進捗表示（10試合ごと）\n",
    "        if (i + 1) % 10 == 0 or i == total_matches - 1:\n",
    "            tqdm.write(f\"[{i+1}/{total_matches}] 累積マッチング率: {match_rate:.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"エラー [{match_id}]: {e}\")\n",
    "        results.append({\"match\": match_id, \"detections\": 0, \"wards\": 0, \"matched\": 0, \"error\": str(e)})\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"処理完了\")\n",
    "print(\"=\"*60)\n",
    "print(f\"処理試合数: {len(results)}\")\n",
    "print(f\"総検出数: {sum(r['detections'] for r in results)}\")\n",
    "print(f\"総ward数: {sum(r['wards'] for r in results)}\")\n",
    "print(f\"マッチング率: {cumulative_matched}/{cumulative_total} ({match_rate:.1f}%)\")\n",
    "print(f\"総処理時間: {total_time/60:.1f}分\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-5: 結果確認\n",
    "print(\"=== 処理結果サマリー ===\")\n",
    "\n",
    "# 各試合のwards_matched.csvを確認\n",
    "match_results = []\n",
    "for match_dir in match_dirs:\n",
    "    matched_csv = match_dir / \"wards_matched.csv\"\n",
    "    if matched_csv.exists():\n",
    "        df = pd.read_csv(matched_csv)\n",
    "        match_results.append({\n",
    "            \"match\": match_dir.name,\n",
    "            \"wards\": len(df),\n",
    "            \"matched\": len(df[df[\"match_status\"] == \"matched\"]) if \"match_status\" in df.columns else 0,\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(match_results)\n",
    "print(f\"\\n処理済み試合: {len(results_df)}\")\n",
    "print(f\"総ward数: {results_df['wards'].sum()}\")\n",
    "print(f\"マッチ済み: {results_df['matched'].sum()}\")\n",
    "\n",
    "if len(results_df) > 0 and results_df['wards'].sum() > 0:\n",
    "    final_match_rate = results_df['matched'].sum() / results_df['wards'].sum() * 100\n",
    "    print(f\"最終マッチング率: {final_match_rate:.1f}%\")\n",
    "\n",
    "display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-6: グリッド特徴量生成（ward_grid.npz）\n",
    "from autoLeague.scoring.grid_generator import generate_ward_grid, save_ward_grid\n",
    "\n",
    "print(\"グリッド特徴量生成を開始...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "grid_success = 0\n",
    "grid_skip = 0\n",
    "\n",
    "for match_dir in tqdm(match_dirs, desc=\"グリッド生成\"):\n",
    "    wards_csv = match_dir / \"wards_matched.csv\"\n",
    "    output_path = match_dir / \"ward_grid.npz\"\n",
    "    \n",
    "    if not wards_csv.exists():\n",
    "        grid_skip += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        grid_data = generate_ward_grid(wards_csv)\n",
    "        save_ward_grid(grid_data, output_path)\n",
    "        grid_success += 1\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"エラー [{match_dir.name}]: {e}\")\n",
    "        grid_skip += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"グリッド生成完了: {grid_success}試合\")\n",
    "print(f\"スキップ: {grid_skip}試合\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fmw0f58jrt5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-7: 次のステップ\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 3 完了\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n出力ファイル:\")\n",
    "print(\"  - detections_raw.csv: 生の検出結果\")\n",
    "print(\"  - wards.csv: クラスタリング後ward情報\")\n",
    "print(\"  - wards_matched.csv: タイムライン統合済み\")\n",
    "print(\"  - ward_grid.npz: グリッド特徴量（32x32x3時間帯）\")\n",
    "print(\"\\n次のステップ:\")\n",
    "print(\"  -> 07_vision_score.ipynb でモデル学習・ヒートマップ可視化\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
