{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‡¯ğŸ‡µ [JP]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Œ æ¦‚è¦\n",
    "\n",
    "å„è©¦åˆï¼ˆ`matchid`ï¼‰ã§**ãƒ”ãƒƒã‚¯ã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ä¸€è¦§ã‚’ä¿å­˜**ã™ã‚‹ä½œæ¥­ã§ã™ã€‚\n",
    "\n",
    "- `.rofl` ãƒªãƒ—ãƒ¬ã‚¤ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãªã  \n",
    "- **CSVãƒ•ã‚¡ã‚¤ãƒ«**ã«ã‚ã‚‹ `matchid` ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿  \n",
    "- å„è©¦åˆã«**å‚åŠ ã—ãŸãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³å**ã‚’æŠ½å‡ºã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: c:\\Users\\lapis\\Desktop\\LoL_WorkSp_win\\pyLoL-_WorkSp\\pyLoL-v2\n",
      "LCU API initialized - port: 57328\n",
      "Current number of replays:  190\n"
     ]
    }
   ],
   "source": [
    "# [1] åˆæœŸè¨­å®š\n",
    "import os, json, csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«ç§»å‹•\n",
    "PROJECT_ROOT = Path(__file__).parent.parent if \"__file__\" in dir() else Path.cwd().parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {Path.cwd()}\")\n",
    "\n",
    "from autoLeague.replays.scraper import ReplayScraper\n",
    "from autoLeague.dataset.riotapi import RiotAPI\n",
    "\n",
    "load_dotenv(\"RiotAPI.env\")\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "\n",
    "# â”€â”€ å…±é€šãƒ‘ã‚¹è¨­å®š(Set common base path) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "USER_HOME = Path(os.environ[\"USERPROFILE\"])    # C:\\Users\\<ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå>\n",
    "\n",
    "# USER_HOMEä¾å­˜ãƒ‘ã‚¹\n",
    "REPLAY_DIR  = USER_HOME / \"Documents/League of Legends/Replays\"\n",
    "SCRAPER_DIR = USER_HOME / \"Desktop/LoL_WorkSp_win/pyLoL-_WorkSp/pyLoL-v2/autoLeague/replays\"\n",
    "\n",
    "# å›ºå®šãƒ‘ã‚¹\n",
    "GAME_DIR       = Path(r\"C:\\Riot Games\\League of Legends\\Game\")\n",
    "SAVE_DIR       = Path(r\"G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\iput æˆæ¥­\\LoL_strorage\\storage\")\n",
    "MATCH_INFO_DIR = Path(r\"G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\iput æˆæ¥­\\LoL_strorage\\match_info\")\n",
    "\n",
    "ra = RiotAPI(api_key=API_KEY)\n",
    "rs = ReplayScraper(game_dir=str(GAME_DIR), replay_dir=str(REPLAY_DIR), save_dir=str(SAVE_DIR), scraper_dir=str(SCRAPER_DIR), replay_speed=27, region=\"KR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] matchid CSVãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆ\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ(List of file paths)\n",
    "file_paths = [\n",
    "    \"matchids/sample_challenger_patch25_13_1.csv\"\n",
    "]\n",
    "\n",
    "# å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§1ã¤ã«ãƒãƒ¼ã‚¸(Read all files and merge into one DataFrame)\n",
    "df_list = [pd.read_csv(path) for path in file_paths]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# é‡è¤‡å‰Šé™¤ï¼šmatchidåˆ—åŸºæº– (Remove duplicates based on 'matchid' column)\n",
    "if 'matchid' in merged_df.columns:\n",
    "    merged_df = merged_df.drop_duplicates(subset='matchid')\n",
    "else:\n",
    "    merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# çµæœä¿å­˜(Save the result)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "merged_df.to_csv(\"data/match_champions(25.24).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLAY_DIRå†…ã®.roflãƒ•ã‚¡ã‚¤ãƒ«æ•°: 190\n",
      "æŠ½å‡ºã•ã‚ŒãŸmatchidæ•°: 190\n",
      "ã‚µãƒ³ãƒ—ãƒ«: ['JP1_556025634', 'JP1_556331133', 'JP1_555629794']\n",
      "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†:\n",
      "  å…ƒã®matchidæ•°: 190\n",
      "  REPLAY_DIRã«å­˜åœ¨ã™ã‚‹matchidæ•°: 190\n",
      "  é™¤å¤–ã•ã‚ŒãŸmatchidæ•°: 0\n"
     ]
    }
   ],
   "source": [
    "# [2.5] REPLAY_DIRã«å­˜åœ¨ã™ã‚‹matchidã®ã¿æ®‹ã™\n",
    "# .roflãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰matchidã‚’æŠ½å‡ºã—ã€CSVã¨ãƒãƒƒãƒãƒ³ã‚°ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "\n",
    "# REPLAY_DIRå†…ã®.roflãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—\n",
    "rofl_files = list(REPLAY_DIR.glob(\"*.rofl\"))\n",
    "print(f\"REPLAY_DIRå†…ã®.roflãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(rofl_files)}\")\n",
    "\n",
    "# .roflãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰matchidã‚’æŠ½å‡ºï¼ˆä¾‹: JP1-552649104.rofl â†’ JP1_552649104ï¼‰\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã—ã¦matchidå½¢å¼ã«åˆã‚ã›ã‚‹\n",
    "rofl_matchids = set()\n",
    "for rofl_path in rofl_files:\n",
    "    filename = rofl_path.stem  # æ‹¡å¼µå­ãªã—ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: JP1-552649104ï¼‰\n",
    "    matchid = filename.replace(\"-\", \"_\")  # ãƒã‚¤ãƒ•ãƒ³â†’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢å¤‰æ›\n",
    "    rofl_matchids.add(matchid)\n",
    "\n",
    "print(f\"æŠ½å‡ºã•ã‚ŒãŸmatchidæ•°: {len(rofl_matchids)}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«: {list(rofl_matchids)[:3]}\")  # ç¢ºèªç”¨ã«3ä»¶è¡¨ç¤º\n",
    "\n",
    "# CSVã‚’èª­ã¿è¾¼ã¿ã€rofl_matchidsã«å­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "csv_path = \"data/match_champions(25.24).csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "original_count = len(df)\n",
    "\n",
    "# matchidåˆ—ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "df_filtered = df[df[\"matchid\"].isin(rofl_matchids)]\n",
    "filtered_count = len(df_filtered)\n",
    "\n",
    "# çµæœã‚’ä¿å­˜\n",
    "df_filtered.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†:\")\n",
    "print(f\"  å…ƒã®matchidæ•°: {original_count}\")\n",
    "print(f\"  REPLAY_DIRã«å­˜åœ¨ã™ã‚‹matchidæ•°: {filtered_count}\")\n",
    "print(f\"  é™¤å¤–ã•ã‚ŒãŸmatchidæ•°: {original_count - filtered_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã™ã‚‹matchidæ•°: 190\n",
      "Total match_ids: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving timeline data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [03:11<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed! Saved: 190, Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# [2.6] ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãƒ»ä¿å­˜\n",
    "# matchidãƒªã‚¹ãƒˆã‹ã‚‰Riot APIã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "\n",
    "TIMELINE_DIR = Path(\"data/timeline\")\n",
    "\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®matchidãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "matchid_list = df_filtered[\"matchid\"].tolist()\n",
    "print(f\"ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã™ã‚‹matchidæ•°: {len(matchid_list)}\")\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»ä¿å­˜\n",
    "ra.save_timeline_data_from_matchlist(\n",
    "    matchids=matchid_list,\n",
    "    output_dir=str(TIMELINE_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# [2.7] ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãƒ»ä¿å­˜\n# matchidãƒªã‚¹ãƒˆã‹ã‚‰Riot APIã§ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n# â€» DatasetBuilderï¼ˆ07_vision_score.ipynbï¼‰ã§å‹æ•—æƒ…å ±å–å¾—ã«ä½¿ç”¨\n\nMATCH_DIR = Path(\"data/match\")\n\n# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®matchidãƒªã‚¹ãƒˆã‚’å–å¾—\nmatchid_list = df_filtered[\"matchid\"].tolist()\nprint(f\"ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹matchidæ•°: {len(matchid_list)}\")\n\n# ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»ä¿å­˜\nra.save_match_json_from_matchlist(\n    matchids=matchid_list,\n    output_dir=str(MATCH_DIR)\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching match data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [05:59<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# [3] ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³åå–å¾—ãƒ»ä¿å­˜\n",
    "ra.save_champnames_from_matches_without_rofl(input_csv_path='data/match_champions(25.24).csv',\n",
    "                                             output_csv_path='data/match_champions_dict(25.24).csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‡¯ğŸ‡µ [JP]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¹ STEP 2: æ¤œå‡ºä¸å¯ã¾ãŸã¯é »ç¹ã«è‚–åƒç”»ãŒå¤‰ã‚ã‚‹ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚’é™¤å¤–\n",
    "\n",
    "- YOLO-11ãƒ™ãƒ¼ã‚¹ã®**æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãèªè­˜ã§ããªã„ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³**  \n",
    "  ï¼ˆâ†’ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œï¼‰  \n",
    "- ã¾ãŸã¯**ãƒ‘ãƒƒã‚·ãƒ–/ã‚¹ã‚­ãƒ«ã§è‚–åƒç”»ãŒé »ç¹ã«å¤‰ã‚ã‚‹ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³**  \n",
    "  ï¼ˆâ†’ ã‚²ãƒ¼ãƒ å†…æ§‹é€ ã®å•é¡Œï¼‰  \n",
    "ã‚’**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é™¤å»ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—**ã§ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‡ºğŸ‡¸ [EN]\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¹ STEP 2: Filtering out problematic champion classes\n",
    "\n",
    "- Excludes champions that the **YOLO-11â€“based model cannot reliably detect**  \n",
    "  (due to training data limitations),  \n",
    "- and champions whose **in-game portraits frequently change**  \n",
    "  (due to passives or skill transformations).  \n",
    "This step filters them **out of the dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ã€YOLOãƒ¢ãƒ‡ãƒ«æœªå¯¾å¿œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã€‘\n",
      "  {'Nilah', 'Kayle', 'Yunara', 'Zaahen'}\n",
      "\n",
      "ã€å¤‰èº«å•é¡Œã®ã‚ã‚‹ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã€‘\n",
      "  {'Neeko', 'Viego', 'Yuumi'}\n",
      "\n",
      "ã€æœ€çµ‚é™¤å¤–ãƒªã‚¹ãƒˆã€‘\n",
      "  {'Nilah', 'Kayle', 'Yunara', 'Zaahen', 'Neeko', 'Viego', 'Yuumi'}\n",
      "  åˆè¨ˆ: 7 ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# [4] YOLOãƒ¢ãƒ‡ãƒ«æœªå¯¾å¿œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚’å‹•çš„ã«æ¤œå‡º\n",
    "# match_champions_dict ã‹ã‚‰å…¨ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³æŠ½å‡º â†’ yolo_supported_champions.csv ã¨æ¯”è¼ƒ\n",
    "\n",
    "YOLO_SUPPORTED_CSV = \"models/yolo_supported_champions.csv\"\n",
    "MATCH_CHAMP_DICT_CSV = \"data/match_champions_dict(25.24).csv\"\n",
    "\n",
    "# 1. YOLOãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã—ã¦ã„ã‚‹ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€\n",
    "yolo_df = pd.read_csv(YOLO_SUPPORTED_CSV)\n",
    "yolo_supported = set(yolo_df[\"champion_name\"].tolist())\n",
    "\n",
    "# 2. è©¦åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚’æŠ½å‡º\n",
    "match_df = pd.read_csv(MATCH_CHAMP_DICT_CSV)\n",
    "champ_cols = [c for c in match_df.columns if c.startswith(\"champion_\")]\n",
    "all_champions_in_matches = set()\n",
    "for col in champ_cols:\n",
    "    all_champions_in_matches.update(match_df[col].dropna().unique())\n",
    "\n",
    "# 3. YOLOãƒ¢ãƒ‡ãƒ«æœªå¯¾å¿œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚’ç‰¹å®š\n",
    "unsupported_by_yolo = all_champions_in_matches - yolo_supported\n",
    "\n",
    "# 4. å¤‰èº«å•é¡Œã®ã‚ã‚‹ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ï¼ˆå¯¾å¿œã¯ã—ã¦ã„ã‚‹ãŒæ¤œå‡ºã«å•é¡Œã‚ã‚Šï¼‰\n",
    "TRANSFORM_PROBLEM_CHAMPIONS = {\"Neeko\", \"Viego\", \"Yuumi\"}\n",
    "\n",
    "# 5. æœ€çµ‚çš„ãªé™¤å¤–ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ\n",
    "EXCLUDED_CHAMPIONS = unsupported_by_yolo | TRANSFORM_PROBLEM_CHAMPIONS\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ã€YOLOãƒ¢ãƒ‡ãƒ«æœªå¯¾å¿œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã€‘\")\n",
    "print(f\"  {unsupported_by_yolo if unsupported_by_yolo else '(ãªã—)'}\")\n",
    "print()\n",
    "print(\"ã€å¤‰èº«å•é¡Œã®ã‚ã‚‹ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã€‘\")\n",
    "print(f\"  {TRANSFORM_PROBLEM_CHAMPIONS}\")\n",
    "print()\n",
    "print(\"ã€æœ€çµ‚é™¤å¤–ãƒªã‚¹ãƒˆã€‘\")\n",
    "print(f\"  {EXCLUDED_CHAMPIONS}\")\n",
    "print(f\"  åˆè¨ˆ: {len(EXCLUDED_CHAMPIONS)} ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é™¤å¤–å¯¾è±¡ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³: {'Nilah', 'Kayle', 'Yunara', 'Zaahen', 'Neeko', 'Viego', 'Yuumi'}\n",
      "é™¤å¤–å¯¾è±¡æ•°: 7\n",
      "Filtered 51 / 111 match_ids saved to data/filtered_matchids(25.24).csv\n"
     ]
    }
   ],
   "source": [
    "# [5] é™¤å¤–ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚’å«ã‚€è©¦åˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "# â€» EXCLUDED_CHAMPIONS ã¯å‰ã®ã‚»ãƒ«ã§å‹•çš„ã«ç”Ÿæˆæ¸ˆã¿\n",
    "\n",
    "CSV_IN  = \"data/match_champions_dict(25.24).csv\"   # å…ƒã®CSV \n",
    "CSV_OUT = \"data/filtered_matchids(25.24).csv\"      # çµæœä¿å­˜ãƒ‘ã‚¹\n",
    "\n",
    "# é™¤å¤–ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºï¼ˆç¢ºèªç”¨ï¼‰\n",
    "print(f\"é™¤å¤–å¯¾è±¡ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³: {EXCLUDED_CHAMPIONS}\")\n",
    "print(f\"é™¤å¤–å¯¾è±¡æ•°: {len(EXCLUDED_CHAMPIONS)}\")\n",
    "\n",
    "ra.filter_matches_by_excluded_champions(CSV_IN, CSV_OUT, EXCLUDED_CHAMPIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total match_ids: 33, Already processed: 0, To process: 33\n",
      "Starting to process 33 match_ids...\n"
     ]
    }
   ],
   "source": [
    "# [6] ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ©ãƒ³ã‚¯æƒ…å ±å–å¾— â†’ å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®— â†’ ã‚­ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º â†’ ä¸Šä½Nè©¦åˆæŠ½å‡º\n",
    "MATCH_AVGSCR_CSV_PATH = \"data/match_avg_score(25.24).csv\"\n",
    "\n",
    "### [6-1] match_champ_dict CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã«ä½¿ç”¨ > match_infoãƒ•ã‚©ãƒ«ãƒ€ã«å€‹åˆ¥ä¿å­˜\n",
    "# æ³¨æ„: regionã¯RiotAPIåˆæœŸåŒ–æ™‚ã«è¨­å®šæ¸ˆã¿ã®ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®šä¸è¦\n",
    "ra.save_summoner_leagueinfo_from_csv(csv_in=CSV_IN, save_folder=MATCH_INFO_DIR, queue_type=\"RANKED_SOLO_5x5\")\n",
    "\n",
    "### match_champ_dict CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã«ä½¿ç”¨\n",
    "# ra.save_summoner_leagueinfo_of_replays(replay_dir = REPLAY_DIR, save_folder = MATCH_INFO_DIR)  # regionã¯RiotAPIåˆæœŸåŒ–æ™‚ã«è¨­å®šæ¸ˆã¿\n",
    "\n",
    "### [6-2] å„è©¦åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼10äººã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦CSVã«ä¿å­˜\n",
    "ra.save_match_avg_score_from_json(match_info_dir=MATCH_INFO_DIR, output_csv_path=MATCH_AVGSCR_CSV_PATH)\n",
    "matchid_list = pd.read_csv(MATCH_AVGSCR_CSV_PATH, usecols=[\"match_id\"]).values.flatten().tolist()\n",
    "\n",
    "### [6-3] å„è©¦åˆã®å„æ™‚é–“å¸¯ï¼ˆåˆ†å˜ä½ï¼‰ã§ã‚­ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆãŒthresholdä»¥ä¸Šã®æ™‚é–“å¸¯ã®ã¿æŠ½å‡ºã—ã¦JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "ra.save_kill_events_from_matchlist(matchids=matchid_list, output_json_path=\"data/kill_events_timeline_from_all_matches.json\", kill_events_threshold=4, max_workers=4)\n",
    "\n",
    "### [6-4] åˆ†ï¼ˆminuteï¼‰10ä»¥ä¸Šã®ã¿æ®‹ã™\n",
    "INPUT_JSON  = \"data/kill_events_timeline_from_all_matches.json\"\n",
    "OUTPUT_JSON = \"data/kill_events_timeline_filtered_10up.json\"\n",
    "\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "filtered = {\n",
    "    mid: [m for m in minutes if m >= 10]\n",
    "    for mid, minutes in data.get(\"data\", {}).items()\n",
    "    if any(m >= 10 for m in minutes)\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"data\": filtered}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ä¿å­˜å®Œäº† â†’ {OUTPUT_JSON}  (è©¦åˆæ•°: {len(filtered)})\")\n",
    "\n",
    "### [6-5] ä¸Šä½Nè©¦åˆã®ã¿æ®‹ã‚‹ã‚ˆã†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "TOP_KILL_EVENTS_JSON = \"data/top_kill_events_timeline_filtered_10up.json\"\n",
    "ra.save_top_matches_by_score(\n",
    "    avg_score_csv_path=MATCH_AVGSCR_CSV_PATH,\n",
    "    data_json_path=OUTPUT_JSON,\n",
    "    output_path=TOP_KILL_EVENTS_JSON,\n",
    "    top_n=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}